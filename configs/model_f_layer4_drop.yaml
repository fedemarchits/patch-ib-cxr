experiment_name: "model_f_layer4_drop"

data:
  json_path: "/workspace/mimic_master_official_split.jsonl"
  image_root: "/datasets/MIMIC-CXR/files"
  batch_size: 48
  num_workers: 4
  image_size: 224
  text_sections: "findings_impression"

model:
  vision_backbone: "hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"
  text_backbone: "hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"

  # Model F ablation: drop at layer 4 instead of layer 6.
  # Purpose: isolate whether Model G's benefit comes from staging the drop
  # or simply from using an earlier drop point.
  # At layer 4, features are positional/textural (weaker semantics than layer 6),
  # so the probe FILIP scorer has less information to work with â€” but 8 upper
  # blocks (vs 6) have more capacity to refine the kept patches.
  use_filip_drop: true

  drop_layer: 4

  # Same final ratio as the layer-6 run for a fair comparison.
  k_ratio: 0.5
  k_ratio_start: 0.75
  k_ratio_anneal_steps: 5000

  use_filip_final: true
  mid_fusion_loss_weights: [0.3, 0.3]
  mid_fusion_warmup_steps: 500

  sparsity_weight: 0.0

  contrastive_mask_weight: 1.0
  contrastive_full_weight: 0.5

training:
  staged_training: true
  warmup_epochs: 1
  warmup_lr: 1.0e-4

  early_stopping_metric: "combined"

  combined_weights:
    recall: 0.7
    auc: 0.3

  eval_auc_every: 1

  epochs: 40
  lr: 1.0e-5
  weight_decay: 0.02
  warmup_steps: 1000
  early_stopping_patience: 6
  print_freq: 50
  device: "cuda"
  use_amp: true
  gradient_accumulation_steps: 4
  freeze_backbone: false
  llrd_factor: 0.85

wandb:
  enable: true
  project: "Thesis-PatchIB"
  run_name: "model-f-layer4-drop-k50"
