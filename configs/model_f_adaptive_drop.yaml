experiment_name: "model_f_adaptive_drop"

data:
  json_path: "/workspace/mimic_master_official_split.jsonl"
  image_root: "/datasets/MIMIC-CXR/files"
  batch_size: 48
  num_workers: 4
  image_size: 224
  text_sections: "findings_impression"

model:
  vision_backbone: "hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"
  text_backbone: "hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"

  # Model F Adaptive: text-conditioned FILIP scoring + adaptive K via STE + sparsity.
  # Unlike Model F (fixed TopK), this lets the model decide per image-text pair
  # how many patches to keep. The threshold at score=0 has a natural interpretation:
  # "keep patches that are positively correlated with the text."
  use_filip_adaptive: true

  # Intermediate ViT layer where FILIP scores are computed.
  # Same as Model F/E for direct comparability.
  drop_layer: 6

  # Temperature for STE binarisation: FILIP scores in [-1,1] scaled to [-T, T].
  # sigmoid(±5) ≈ 0.007/0.993 → near-binary selection.
  # Lower T (e.g., 2) = softer selection, more gradient flow but noisier mask.
  drop_temperature: 5.0

  # Gumbel-Sigmoid (optional): replaces plain STE with temperature-annealed noise.
  # Start with false — switch to true if training is unstable.
  use_gumbel: false
  # gumbel_tau_start: 1.0
  # gumbel_tau_end: 0.1
  # gumbel_tau_anneal_steps: 5000

  # Sparsity loss: (sigmoid(scaled_scores).mean() - target_ratio)^2
  # Drives the adaptive K toward keeping ~50% of patches on average.
  # The model can keep more for complex phrases, fewer for simple ones.
  mask_ratio: 0.5        # target_ratio for SparsityLoss
  sparsity_weight: 5.0   # balance vs contrastive (~3.0-6.0 loss at convergence)
  sparsity_warmup_steps: 500   # let backbone warm up first

  # Probe FILIP loss: all 196 patches at drop_layer (key training signal).
  # Trains backbone blocks 0..drop_layer-1 and probe projections.
  mid_fusion_loss_weights: [0.3]
  mid_fusion_warmup_steps: 500

  # Dual contrastive losses:
  #   contrastive_mask_weight: weight on the main full-CLS contrastive (text-independent)
  #   contrastive_full_weight: weight on the auxiliary masked-mean-pool contrastive
  #     (text-conditioned via FILIP mask, trains scorer end-to-end via img_emb_full)
  contrastive_mask_weight: 1.0
  contrastive_full_weight: 0.5   # Auxiliary signal on masked-mean-pool embedding

training:
  staged_training: true
  warmup_epochs: 1
  warmup_lr: 1.0e-4

  early_stopping_metric: "combined"

  combined_weights:
    recall: 0.7
    auc: 0.3

  eval_auc_every: 1

  epochs: 40
  lr: 1.0e-5
  weight_decay: 0.02
  warmup_steps: 1000
  early_stopping_patience: 6
  print_freq: 50
  device: "cuda"
  use_amp: true
  gradient_accumulation_steps: 4
  freeze_backbone: false
  llrd_factor: 0.85

wandb:
  enable: true
  project: "Thesis-PatchIB"
  run_name: "model-f-adaptive-drop-t5"
