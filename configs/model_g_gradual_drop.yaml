experiment_name: "model_g_gradual_drop"

data:
  json_path: "/workspace/mimic_master_official_split.jsonl"
  image_root: "/datasets/MIMIC-CXR/files"
  batch_size: 48
  num_workers: 4
  image_size: 224
  text_sections: "findings_impression"

model:
  vision_backbone: "hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"
  text_backbone: "hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"

  # Model G: 2-stage gradual FILIP patch dropping.
  # Stage 1 (coarse): FILIP scoring at layer 4 removes obvious background (196 → K1).
  # Stage 2 (fine):   FILIP scoring at layer 9 uses richer semantic features to
  #                   select disease-relevant patches from K1 survivors (K1 → K2).
  # Net sparsity ≈ 50% (K2 ≈ 98 patches), matching Model F for fair comparison.
  use_gradual_drop: true

  # Drop points (1-indexed layer numbers):
  #   drop_layer_1=4  — early features good for background removal (position, texture)
  #   drop_layer_2=9  — richer semantic features for disease-relevant patch selection
  drop_layer_1: 4
  drop_layer_2: 9

  # Stage 1: 196 → K1 patches.
  # Final k_ratio_1=0.75 → K1=147. Annealed from 0.90 (176 patches) over 5000 steps.
  k_ratio_1: 0.75
  k_ratio_1_start: 0.9

  # Stage 2: K1 → K2 patches.
  # Final k_ratio_2=0.67 → K2≈98 (50% of 196). Annealed from 0.85 (≈125 patches).
  k_ratio_2: 0.67
  k_ratio_2_start: 0.85

  k_ratio_anneal_steps: 5000

  # FILIP losses: three entries in the local_features list returned by ModelG.
  #   [0] = Stage 1 probe FILIP at drop_layer_1 on ALL 196 patches (before drop)
  #         — trains backbone blocks 0..3 and probe_patch_proj_1/probe_token_proj_1
  #   [1] = Stage 2 probe FILIP at drop_layer_2 on K1 patches (before drop)
  #         — trains backbone blocks 4..8 and probe_patch_proj_2/probe_token_proj_2
  #   [2] = Final FILIP on K2 selected patches after upper-block processing
  #         — trains backbone blocks 9..11 and patch_proj/token_proj
  use_filip_final: true
  mid_fusion_loss_weights: [0.3, 0.3, 0.3]
  mid_fusion_warmup_steps: 500   # Let backbone warm up before FILIP losses activate

  # Sparsity weight: 0.0 — TopK guarantees exact sparsity at both stages.
  sparsity_weight: 0.0

  # Dual contrastive losses:
  #   contrastive_mask_weight: weight on main full-CLS contrastive (text-independent)
  #   contrastive_full_weight: weight on auxiliary dropped-CLS contrastive
  #     (text-conditioned, trains both FILIP scorers end-to-end via img_emb_full tuple)
  contrastive_mask_weight: 1.0
  contrastive_full_weight: 0.5

training:
  staged_training: true
  warmup_epochs: 1
  warmup_lr: 1.0e-4

  early_stopping_metric: "combined"

  combined_weights:
    recall: 0.7
    auc: 0.3

  eval_auc_every: 1

  epochs: 40
  lr: 1.0e-5
  weight_decay: 0.02
  warmup_steps: 1000
  early_stopping_patience: 6
  print_freq: 50
  device: "cuda"
  use_amp: true
  gradient_accumulation_steps: 4
  freeze_backbone: false
  llrd_factor: 0.85

wandb:
  enable: true
  project: "Thesis-PatchIB"
  run_name: "model-g-gradual-drop"
