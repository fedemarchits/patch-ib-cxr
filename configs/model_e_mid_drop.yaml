experiment_name: "model_e_mid_drop"

data:
  json_path: "/workspace/mimic_master_official_split.jsonl"
  image_root: "/datasets/MIMIC-CXR/files"
  batch_size: 48
  num_workers: 4
  image_size: 224
  text_sections: "findings_impression"

model:
  vision_backbone: "hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"
  text_backbone: "hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"

  # Model E: Intra-ViT patch dropping
  use_mid_drop: true
  drop_layer: 6          # Drop after ViT block 6 (of 12) — midpoint, semantically meaningful
  k_ratio: 0.5           # Final ratio: keep 98/196 patches
  k_ratio_start: 0.75    # Anneal from 75% -> 50% over first 5000 steps
  k_ratio_anneal_steps: 5000

  # Sparsity regularization on scorer logits (light — Top-K guarantees exact sparsity)
  sparsity_weight: 1.0
  sparsity_warmup_steps: 500

  # FILIP local loss on K selected patches after upper ViT processing
  # These patches are semantically refined by blocks 6-11 — ideal for token alignment
  use_filip_local: true
  filip_local_weight: [0.3]
  filip_local_warmup_steps: 500

  # Single contrastive loss on the reduced-sequence CLS token
  contrastive_mask_weight: 1.0
  contrastive_full_weight: 1.0  # Unused (img_emb_full=None for Model E)

training:
  staged_training: true
  warmup_epochs: 1
  warmup_lr: 1.0e-4

  early_stopping_metric: "combined"

  combined_weights:
    recall: 0.7
    auc: 0.3

  eval_auc_every: 1

  epochs: 40
  lr: 1.0e-5
  weight_decay: 0.02
  warmup_steps: 1000
  early_stopping_patience: 6
  print_freq: 50
  device: "cuda"
  use_amp: true
  gradient_accumulation_steps: 4
  freeze_backbone: false
  llrd_factor: 0.85

wandb:
  enable: true
  project: "Thesis-PatchIB"
  run_name: "model-e-mid-drop-k50"
