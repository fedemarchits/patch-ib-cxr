experiment_name: "model_d_filip_local"

data:
  json_path: "/workspace/mimic_master_official_split.jsonl"
  image_root: "/datasets/MIMIC-CXR/files"
  batch_size: 48
  num_workers: 4
  image_size: 224
  text_sections: "findings_impression"

model:
  vision_backbone: "hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"
  text_backbone: "hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"

  # TopK patch selection (MaskHead)
  use_masking: true
  use_topk_masking: true
  k_ratio: 0.5              # Keep 98/196 patches at convergence
  k_ratio_start: 0.75       # Anneal 75% -> 50% over first 5000 steps
  k_ratio_anneal_steps: 5000

  # FILIP local alignment on the K selected patches ONLY.
  # Gradient flows: FILIP loss -> selected patches -> STE mask -> importance logits.
  # This is the primary supervision signal for the MaskHead.
  use_local_alignment: true
  local_alignment_loss_type: "filip"   # InfoNCE, co-scales with global contrastive
  local_alignment_weight: 1.0          # Weight=1 = same scale as each contrastive term
  local_alignment_warmup_steps: 500

  # No mid-fusion: masking post-ViT features would be contaminated by cross-attention
  use_mid_fusion: false

  # Global CLS contrastive: trains backbone representations (MaskHead not involved)
  contrastive_full_weight: 1.0
  # Masked pool contrastive: secondary signal via STE mean-pool path
  contrastive_mask_weight: 1.0
  # Consistency removed: teaches backbone that any patch subset ≈ CLS → uniformity collapse
  consistency_weight: 0.0

  sparsity_weight: 1.0
  sparsity_warmup_steps: 500

training:
  staged_training: true
  warmup_epochs: 1
  warmup_lr: 1.0e-4

  early_stopping_metric: "combined"
  combined_weights:
    recall: 0.7
    auc: 0.3
  eval_auc_every: 1

  epochs: 40
  lr: 1.0e-5
  weight_decay: 0.02
  warmup_steps: 1000
  early_stopping_patience: 6
  print_freq: 50
  device: "cuda"
  use_amp: true
  gradient_accumulation_steps: 4
  freeze_backbone: false
  llrd_factor: 0.85

wandb:
  enable: true
  project: "Thesis-PatchIB"
  run_name: "model-d-filip-local"
